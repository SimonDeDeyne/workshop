grid.arrange(fig.pumpkin, fig.orchid, ncol = 2)
X.con = read.delim('./rawdata/english.concreteness.Brysbaert2016.csv',stringsAsFactors = F) %>% as_tibble()
X.con = inner_join(X.ct,X.con,by = c('names'= 'Word'))
rs  = corrTable(X.con %>% select(-names,-Dom_Pos),rmethod='spearman',absolute = TRUE)
DT::datatable(cbind(ct_pr$names,round(ct_pr %>% select(-names),5)))
DT::datatable(cbind(ct_pr$names,round(ct_pr %>% select(-names),5)))
DT::datatable(cbind(ct_b$names,round(ct_b %>% select(-names),5)))
# 758F2C
X.ct = left_join(ct_k,ct_pr,by = 'names')
m_ct = lm(strength_in ~ pr_70,data = X.ct)
X.ct$cooks = cooks.distance(m_ct)
outlier = X.ct %>% slice_max(n = 30,order_by = abs(cooks))
fig.pr = ggplot(X.ct,aes(x = strength_in,y = pr_70)) +
geom_point(size = 0.3,alpha = 0.3,colour = '#DD4814') +
geom_smooth(method = "lm", formula= y ~ x, se = FALSE,color = 'grey',size = 0.5) +
geom_text_repel(data = X.ct %>% filter(names %in% outlier$names),
aes(label = names),segment.color = '#DD4814',segment.alpha=0.5,max.overlaps = 200) +
theme(aspect.ratio=1) +
ggtitle('PageRank centrality')
m_ct = lm(strength_in ~ eigen,data = X.ct)
X.ct$cooks = cooks.distance(m_ct)
outlier = X.ct %>% slice_max(n = 30,order_by = abs(cooks))
fig.eig = ggplot(X.ct,aes(x = strength_in,y = eigen)) +
geom_point(size = 0.3,alpha = 0.3,colour = '#DD4814') +
geom_smooth(method = "lm", formula= y ~ x, se = FALSE,color = 'grey',size = 0.5) +
geom_text_repel(data = X.ct %>% filter(names %in% outlier$names),
aes(label = names),segment.color = '#DD4814',segment.alpha=0.5,max.overlaps = 200) +
theme(aspect.ratio=1) +
ggtitle('Eigencentrality')
grid.arrange(fig.eig, fig.pr, ncol = 2)
grid.arrange(fig1, fig2, ncol = 2)
grid.arrange(fig.pumpkin, fig.orchid, ncol = 2)
fig.conc
X.con = read.delim('./rawdata/english.concreteness.Brysbaert2016.csv',stringsAsFactors = F) %>% as_tibble()
X.con = inner_join(X.ct,X.con,by = c('names'= 'Word'))
rs  = corrTable(X.con %>% select(-names,-Dom_Pos),rmethod='spearman',absolute = TRUE)
X.con = read.delim('./rawdata/english.concreteness.Brysbaert2016.csv',stringsAsFactors = F) %>% as_tibble()
X.con = inner_join(X.ct,X.con,by = c('names'= 'Word'))
rs  = corrTable(X.con %>% select(-names,-Dom_Pos),rmethod='spearman',absolute = TRUE)
library(SWOW)
library(htmltools)
library(SWOW)
library(knitr)
library(gridExtra)
X.con = read.delim('./rawdata/english.concreteness.Brysbaert2016.csv',stringsAsFactors = F) %>% as_tibble()
X.con = inner_join(X.ct,X.con,by = c('names'= 'Word'))
rs  = corrTable(X.con %>% select(-names,-Dom_Pos),rmethod='spearman',absolute = TRUE)
fig.conc = ggplot(rs %>% filter(var1 =='Conc.M',
var2 %in% c('nb2_out','nb3_out','degree_out','betweenness','betweennessU',
'eigen','pr_70','strength_in','cc','SUBTLEX')),
aes(x=reorder(var2, var2, function(x) length(x)), y=r)) +
geom_bar(stat="identity", aes(fill = sign)) +
scale_fill_manual(values=c("#DD4814", "#758F2C")) +
geom_errorbar(aes(ymin=cl, ymax=cu), width=.2)  + theme_minimal() + xlab('Centrality Measure') +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
fig.conc
# Load Calgary Semantic Decision Project Data
X.csdp = read.delim('./rawdata/CalgarySemanticDecision/CalgarySemanticDecisionSummary.csv',sep = '\t') %>% as_tibble() %>%
select(word,wordType,RT = zRTclean_mean,accuracy = ACC,concreteRating)
X.csdp = inner_join(X.csdp,X.ct,by = c('word' = 'names')) %>% filter(complete.cases(.))
DT::datatable(as.data.frame(round(cor(X.csdp %>% select(-word,-wordType),method = 'spearman')[,c('RT','accuracy','concreteRating')],3)))
rs.csdp  = corrTable(X.csdp %>% select(-word,-wordType),rmethod='spearman',absolute = TRUE)
fig.csdp = ggplot(rs.csdp %>% filter(var1 =='RT',
var2 %in% c('nb2_out','nb3_out','degree_out','cc','betweenness','betweennessU',
'eigen','pr_70','strength_in','SUBTLEX')),
aes(x=reorder(var2, var2, function(x) length(x)), y=r)) +
geom_bar(stat="identity", aes(fill = sign)) +
scale_fill_manual(values=c("#DD4814", "#758F2C")) +
geom_errorbar(aes(ymin=cl, ymax=cu), width=.2)  + theme_minimal() + xlab('Centrality Measure') +
theme(axis.text.x = element_text(angle = 45,hjust = 1))
fig.csdp
# Correct the skew in nb2_in
m.csdp = lm(RT ~ log(nb2_in+1) + (nb2_out), data = X.csdp)
X.csdp
X.ct = left_join(ct_nb,ct_k,by = 'names') %>% left_join(.,ct_pr,by = 'names') %>% left_join(.,ct_b,by ='names')
m_ct = lm(strength_in ~ betweenness,data = X.ct)
X.ct$cooks = cooks.distance(m_ct)
outlier = X.ct %>% slice_max(n = 40,order_by = abs(cooks))
fig1 = ggplot(X.ct,aes(x = strength_in,y = betweenness)) +
geom_point(size = 0.3,alpha = 0.3,colour = '#DD4814') +
geom_smooth(method = "lm", formula= y ~ x, se = FALSE,color = 'grey',size = 0.5) +
geom_text_repel(data = X.ct %>% filter(names %in% outlier$names),
aes(label = names),segment.color = '#DD4814',segment.alpha=0.5,max.overlaps = 100,size =3) +
theme(aspect.ratio=1) +
ggtitle('Betweenness')
# Let's zoom in on the low range
outlier = X.ct %>% filter(strength_in < 5) %>%  slice_max(n = 40,order_by = abs(cooks))
fig2 = ggplot(X.ct %>% filter(strength_in<5),aes(x = strength_in,y = betweenness)) +
geom_point(size = 0.3,alpha = 0.3,colour = '#DD4814') +
geom_smooth(method = "lm", formula= y ~ x, se = FALSE,color = 'grey',size = 0.5) +
geom_text_repel(data = X.ct %>% filter(names %in% outlier$names),
aes(label = names),segment.color = '#DD4814',segment.alpha=0.5,max.overlaps = 100,size = 3) +
theme(aspect.ratio=1) +
ggtitle('Betweenness (Zoom)')
X.con = read.delim('./rawdata/english.concreteness.Brysbaert2016.csv',stringsAsFactors = F) %>% as_tibble()
X.con = inner_join(X.ct,X.con,by = c('names'= 'Word'))
rs  = corrTable(X.con %>% select(-names,-Dom_Pos),rmethod='spearman',absolute = TRUE)
fig.conc = ggplot(rs %>% filter(var1 =='Conc.M',
var2 %in% c('nb2_out','nb3_out','degree_out','betweenness','betweennessU',
'eigen','pr_70','strength_in','cc','SUBTLEX')),
aes(x=reorder(var2, var2, function(x) length(x)), y=r)) +
geom_bar(stat="identity", aes(fill = sign)) +
scale_fill_manual(values=c("#DD4814", "#758F2C")) +
geom_errorbar(aes(ymin=cl, ymax=cu), width=.2)  + theme_minimal() + xlab('Centrality Measure') +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
fig.conc
# Load Calgary Semantic Decision Project Data
X.csdp = read.delim('./rawdata/CalgarySemanticDecision/CalgarySemanticDecisionSummary.csv',sep = '\t') %>% as_tibble() %>%
select(word,wordType,RT = zRTclean_mean,accuracy = ACC,concreteRating)
X.csdp = inner_join(X.csdp,X.ct,by = c('word' = 'names')) %>% filter(complete.cases(.))
DT::datatable(as.data.frame(round(cor(X.csdp %>% select(-word,-wordType),method = 'spearman')[,c('RT','accuracy','concreteRating')],3)))
rs.csdp  = corrTable(X.csdp %>% select(-word,-wordType),rmethod='spearman',absolute = TRUE)
fig.csdp = ggplot(rs.csdp %>% filter(var1 =='RT',
var2 %in% c('nb2_out','nb3_out','degree_out','cc','betweenness','betweennessU',
'eigen','pr_70','strength_in','SUBTLEX')),
aes(x=reorder(var2, var2, function(x) length(x)), y=r)) +
geom_bar(stat="identity", aes(fill = sign)) +
scale_fill_manual(values=c("#DD4814", "#758F2C")) +
geom_errorbar(aes(ymin=cl, ymax=cu), width=.2)  + theme_minimal() + xlab('Centrality Measure') +
theme(axis.text.x = element_text(angle = 45,hjust = 1))
fig.csdp
# Correct the skew in nb2_in
m.csdp = lm(RT ~ log(nb2_in+1) + (nb2_out), data = X.csdp)
print(summary(m.csdp))
X.ecd = read.csv('./rawdata/EnglishLexiconProject/ELPzScores.csv',stringsAsFactors = F) %>% as_tibble() %>%
select(word = Word,length = Length, ON = Ortho_N,prevalence = Prevalence,ECP_RT,zRT = zECP_24,WF = SubtlexZipf)
X.csdp2 = inner_join(X.csdp,X.ecd,by ='word')
rs.ecd  = corrTable(X.csdp2 %>% select(-word,-wordType),rmethod='spearman',absolute = TRUE)
fig.ecd = ggplot(rs.ecd %>% filter(var1 =='zRT',
var2 %in% c('nb2_out','nb3_out','degree_out','cc','betweenness','betweennessU',
'eigen','pr_70','strength_in','WF')),
aes(x=reorder(var2, var2, function(x) length(x)), y=r)) +
geom_bar(stat="identity", aes(fill = sign)) +
scale_fill_manual(values=c("#DD4814", "#758F2C")) +
geom_errorbar(aes(ymin=cl, ymax=cu), width=.2)  + theme_minimal() + xlab('Centrality Measure') +
theme(axis.text.x = element_text(angle = 45,hjust = 1))
fig.ecd
X.ecd = read.csv('./rawdata/EnglishLexiconProject/ELPzScores.csv',stringsAsFactors = F) %>% as_tibble() %>%
select(word = Word,length = Length, ON = Ortho_N,prevalence = Prevalence,ECP_RT,zRT = zECP_24,WF = SubtlexZipf)
X.csdp2 = inner_join(X.csdp,X.ecd,by ='word')
rs.ecd  = corrTable(X.csdp2 %>% select(-word,-wordType),rmethod='spearman',absolute = TRUE)
fig.ecd = ggplot(rs.ecd %>% filter(var1 =='zRT',
var2 %in% c('degree_out','cc','betweenness','betweennessU',
'eigen','pr_70','strength_in','WF')),
aes(x=reorder(var2, var2, function(x) length(x)), y=r)) +
geom_bar(stat="identity", aes(fill = sign)) +
scale_fill_manual(values=c("#DD4814", "#758F2C")) +
geom_errorbar(aes(ymin=cl, ymax=cu), width=.2)  + theme_minimal() + xlab('Centrality Measure') +
theme(axis.text.x = element_text(angle = 45,hjust = 1))
fig.ecd
library(SWOW)
library(knitr)
options(scipen = 999)
G.usf  = importGraph(USF)
plot(igraph::simplify(G.usf),layout=igraph::layout_with_lgl, vertex.color="green",vertex.size=1,vertex.label = NA,edge.arrow.size = 0,edge.width = 0.2)
targetWord = 'pizza'
G.sub = igraph::make_ego_graph(G.usf, order = 2, nodes = targetWord, mode = c("all","out", "in"), mindist = 0)[[1]]
message(igraph::vcount(G.sub))
# Add communities
#G.sub = addCommunity(as_tbl_graph(G.sub),1)
fig.com = ggraph(G.sub,layout = "drl")+
geom_edge_link0(aes(edge_width = weight),edge_colour = "grey66",alpha = 0.1,size = 0.1)+
geom_node_point(aes(fill = community,color = community),shape = 'circle', alpha = 0.8, stroke = 0, size = 1) +
scale_fill_viridis_d(option = 'A') +
scale_edge_width(range = c(0.02,0.4)) +
theme_graph() +
coord_fixed() +
theme(legend.position = "none")
fig.com
# Add communities
G.sub = addCommunity(as_tbl_graph(G.sub),1)
fig.com = ggraph(G.sub,layout = "drl")+
geom_edge_link0(aes(edge_width = weight),edge_colour = "grey66",alpha = 0.1,size = 0.1)+
geom_node_point(aes(fill = community,color = community),shape = 'circle', alpha = 0.8, stroke = 0, size = 1) +
scale_fill_viridis_d(option = 'A') +
scale_edge_width(range = c(0.02,0.4)) +
theme_graph() +
coord_fixed() +
theme(legend.position = "none")
fig.com
fig.com
# Add communities
G.sub = addCommunity(as_tbl_graph(G.sub),1)
fig.com = ggraph(G.sub,layout = "drl")+
geom_edge_link0(aes(edge_width = weight),edge_colour = "grey66",alpha = 0.1)+
geom_node_point(aes(fill = community,color = community),shape = 'circle', alpha = 0.8, stroke = 0, size = 1) +
scale_fill_viridis_d(option = 'A') +
scale_edge_width(range = c(0.02,0.4)) +
theme_graph() +
coord_fixed() +
theme(legend.position = "none")
targetword = 'Italian'
G.ego = extractEgoGraph(G.usf,v = targetword,0.01)
message('# nodes: ', igraph::vcount(G.ego))
fig.ego = plotEgoGraph(G.ego,PF = FALSE,layoutAlg = 'stress')
targetword = 'Italian'
G.ego = extractEgoGraph(G.usf,v = targetword,0.01)
fig.ego = plotEgoGraph(G.ego,PF = FALSE,layoutAlg = 'stress')
#message('# nodes: ', igraph::vcount(G.ego))
print(fig.ego)
library(SWOW)
library(SWOW)
library(knitr)
options(scipen = 999)
print(fig.ego)
print(fig.ego)
print(fig.ego)
print(fig.ego)
fig.com
fig.com
# Plot the entire network with igraph
plot(igraph::simplify(G.usf),layout=igraph::layout_with_lgl, vertex.color="green",vertex.size=1,vertex.label = NA,edge.arrow.size = 0,edge.width = 0.2)
# Plot the entire network with igraph
plot(igraph::simplify(G.usf),layout=igraph::layout_with_lgl, vertex.color="black",vertex.size=1,vertex.label = NA,edge.arrow.size = 0,edge.width = 0.2)
print(fig.ego)
fig.circle  = plotCirclepack(G.ego,20)
?viridis
library(SWOW)
fig.circle  = plotCirclepack(G.ego,20)
fig.circle
# Plot the entire network with igraph
fig.usf = plot(igraph::simplify(G.usf),layout=igraph::layout_with_lgl, vertex.color="black",vertex.size=1,vertex.label = NA,edge.arrow.size = 0,edge.width = 0.2)
# Plot the entire network with igraph
fig.usf
targetWord = 'pizza'
G.sub = igraph::make_ego_graph(G.usf, order = 2, nodes = targetWord, mode = "all", mindist = 0)[[1]]
# Add communities
G.sub = addCommunity(as_tbl_graph(G.sub),1)
# Here we use the ggraph package which offers more layout options
fig.com = ggraph(G.sub,layout = "drl")+
geom_edge_link0(aes(edge_width = weight),edge_colour = "grey66",alpha = 0.1)+
geom_node_point(aes(fill = community,color = community),shape = 'circle', alpha = 0.8, stroke = 0, size = 1) +
scale_fill_viridis_d(option = 'A') +
scale_edge_width(range = c(0.02,0.4)) +
theme_graph() +
coord_fixed() +
theme(legend.position = "none")
# Add clusters or communities
G.sub = addCommunity(as_tbl_graph(G.sub),1)
# Here we use the ggraph package which offers more layout options
fig.com = ggraph(G.sub,layout = "drl")+
geom_edge_link0(aes(edge_width = weight),edge_colour = "grey66",alpha = 0.1)+
geom_node_point(aes(fill = community,color = community), alpha = 0.8, stroke = 0, size = 1) +
scale_fill_viridis_d(option = 'A') +
scale_edge_width(range = c(0.02,0.4)) +
theme_graph() +
coord_fixed() +
theme(legend.position = "none")
fig.com
targetword = 'Italian'
G.ego = extractEgoGraph(G.usf,v = targetword,0.01)
fig.ego = plotEgoGraph(G.ego,PF = FALSE,layoutAlg = 'stress')
print(fig.ego)
fig.pf = plotEgoGraph(G.ego,PF = TRUE,q = 2,r = Inf,layoutAlg = 'stress')
fig.pf
fig.pf = plotEgoGraph(G.ego,PF = TRUE,q = 2,r = Inf,layoutAlg = 'stress')
fig.pf
fig.pf
fig.pf
# Plot the entire network with igraph
plot(igraph::simplify(G.usf),layout=igraph::layout_with_lgl, vertex.color="black",vertex.size=1,vertex.label = NA,edge.arrow.size = 0,edge.width = 0.2)
G.usf  = importGraph(USF)
print(G.usf)
X.ratings = read.delim('../SWOW/rawdata/similarityRatings/simRatings.csv',sep = ',') %>% as_tibble()
# Extract unique word list from the ratings that are also in the list of vertices
wordlist = unique(c(as.character(X.ratings$WordA),as.character(X.ratings$WordB)))
print(X.ratings %>% group_by(dataset) %>% summarise(n = n()))
vNames = igraph::V(G.strength)$name
X.ratings = read.delim('../SWOW/rawdata/similarityRatings/simRatings.csv',sep = ',') %>% as_tibble()
# Extract unique word list from the ratings that are also in the list of vertices
wordlist = unique(c(as.character(X.ratings$WordA),as.character(X.ratings$WordB)))
print(X.ratings %>% group_by(dataset) %>% summarise(n = n()))
vNames = igraph::V(G.usf)$name
wordlist = intersect(wordlist,vNames)
igraph::get.all.shortest.paths(G.ppmi,'red',to = 'pasta')$res
G.strength  = importGraph(USF)
# This function works on stochastic (rows sum to 1) weighted adjacency matrix P for unipartite graphs
# PPMI = function(P){
#   N   = dim(P)[1]
#   D   = Matrix::Diagonal(x = 1/(Matrix::colSums(P)/N))
#   P   = P %*% D
#   P@x = log2(P@x)
#   P2  = pmax(P,0)
#   return(P2)
# }
# Calculate the joint probabilities
N_cue = USF %>% group_by(cue) %>% tally() %>% nrow()
N_resp = sum(USF$R1)
USF = USF %>% mutate(P_rc = R1.Strength / N_cue)
USF = USF %>% group_by(cue) %>% mutate(P_c = n / N_cue)
USF = USF %>% group_by(response) %>% mutate(n_R1 = sum(R1))
USF = USF %>% group_by(response) %>% mutate(P_r = n_R1/N_resp)
USF = USF %>% mutate(R1.PPMI = log2(P_rc/(P_c*P_r)))
USF = USF %>% select(-P_rc,-P_c,-n_R1,-P_r)
# Convert to probabilities
#USF = USF %>% mutate(n_PPMI
G.ppmi = getPPMI(G.strength)
#igraph::E(G.ppmi)[igraph::incident(G.ppmi,'pizza', mode = 'out')]
#igraph::E(G.ppmi)[igraph::incident(G.ppmi,'pizza', mode = 'out')]$weight
#igraph::similarity(G.ppmi,v = c('pizza','pasta'),mode = 'out',method = 'Dice')
#cosineRows(G.ppmi['pizza',],G.ppmi['pasta',])
print(cosineMatrix(G.ppmi[c('pizza','pasta','chocolate','red'),]))
nb_pasta = igraph::neighbors(G.ppmi,'pasta')
nb_red = igraph::neighbors(G.ppmi,'red')
print(nb_pasta)
print(nb_red)
igraph::intersection(nb_pasta,nb_red)
igraph::get.all.shortest.paths(G.ppmi,'red',to = 'pasta')$res
igraph::get.all.shortest.paths(G.ppmi,'red',to = 'pasta')$res[[1]]
cue_pizza = USF %>% filter(cue=='pizza') %>% arrange(-R1.PPMI)
DT::datatable(cue_pizza)
nb_pasta = igraph::neighbors(G.ppmi,'pasta')
nb_red = igraph::neighbors(G.ppmi,'red')
print(nb_pasta)
print(nb_red$names)
nb_pasta = igraph::neighbors(G.ppmi,'pasta')
nb_red = igraph::neighbors(G.ppmi,'red')
print(nb_pasta)
print(nb_red$names[1])
nb_pasta = igraph::neighbors(G.ppmi,'pasta')
nb_red = igraph::neighbors(G.ppmi,'red')
#print(nb_pasta)
print(nb_red[1]$names)
nb_pasta = igraph::neighbors(G.ppmi,'pasta')
nb_red = igraph::neighbors(G.ppmi,'red')
#print(nb_pasta)
print(nb_red[1])
nb_pasta = igraph::neighbors(G.ppmi,'pasta')
nb_red = igraph::neighbors(G.ppmi,'red')
#print(nb_pasta)
print(nb_red$names)
nb_pasta = igraph::neighbors(G.ppmi,'pasta')
nb_red = igraph::neighbors(G.ppmi,'red')
print(nb_pasta)
print(nb_red)
# Look up forward and backward associative strength
# Note: here we're indexing on a full graph, which means this needs to fit in memory
tmp.strength = as.array(igraph::as_adjacency_matrix(G.strength,attr='weight',names = TRUE))
tmp.ppmi = as.array(igraph::as_adjacency_matrix(G.ppmi,attr='weight',names = TRUE))
tmp.rw  = as.array(igraph::as_adjacency_matrix(G.rw,attr='weight',names = TRUE))
# also see https://github.com/cran/centiserve/blob/master/R/katzcent.R
alpha = 0.4
P = igraph::as_adjacency_matrix(G.ppmi,attr='weight',names = TRUE)
I = diag(1, dim(P)[1]);
K = solve(I - alpha*P);
P = PPMI(K)
diag(P) = 0
P = normalize(P,'l1')
G.rw = igraph::graph_from_adjacency_matrix(Matrix::as.matrix(P),weighted = TRUE)
message(igraph::edge_density(G.ppmi))
message(igraph::edge_density(G.rw))
# Make it fancy: http://haozhu233.github.io/kableExtra/use_kableExtra_with_formattable.html
G.rw['pasta','red']
# Neighboring vertices to pasta from the Katz Index graph G.rw
V.rw = data.frame(strength = G.rw['pasta',names(igraph::neighbors(G.rw,'pasta'))]) %>%
rownames_to_column(. , var = 'name') %>% as_tibble()
# Neighboring vertices to pasta from the PPMI graph G.ppmi
V.ppmi = data.frame(strength = G.ppmi['pasta',names(igraph::neighbors(G.ppmi,'pasta'))]) %>%
rownames_to_column(. , var = 'name')  %>% as_tibble()
# Combine to compare
V.joint = full_join(V.rw,V.ppmi,by  = 'name', suffix = c('.rw','.ppmi')) %>%
as_tibble() %>% select(name,strength.rw,strength.ppmi) %>% arrange(-strength.rw)
print(V.joint)
# Make it fancy: http://haozhu233.github.io/kableExtra/use_kableExtra_with_formattable.html
G.rw['pasta','red']
# Neighboring vertices to pasta from the Katz Index graph G.rw
V.rw = data.frame(strength = G.rw['pasta',names(igraph::neighbors(G.rw,'pasta'))]) %>%
rownames_to_column(. , var = 'name') %>% as_tibble()
# Neighboring vertices to pasta from the PPMI graph G.ppmi
V.ppmi = data.frame(strength = G.ppmi['pasta',names(igraph::neighbors(G.ppmi,'pasta'))]) %>%
rownames_to_column(. , var = 'name')  %>% as_tibble()
# Combine to compare
V.joint = full_join(V.rw,V.ppmi,by  = 'name', suffix = c('.rw','.ppmi')) %>%
as_tibble() %>% select(name,strength.rw,strength.ppmi) %>% arrange(-strength.rw)
DT::datatable(V.joint)
X.ratings = read.delim('../SWOW/rawdata/similarityRatings/simRatings.csv',sep = ',') %>% as_tibble()
# Extract unique word list from the ratings that are also in the list of vertices
wordlist = unique(c(as.character(X.ratings$WordA),as.character(X.ratings$WordB)))
print(X.ratings %>% group_by(dataset) %>% summarise(n = n()))
vNames = igraph::V(G.usf)$name
wordlist = intersect(wordlist,vNames)
pairs = X.ratings %>% mutate(WordA = as.character(WordA),WordB = as.character(WordB)) %>%
filter(WordA %in% vNames,WordB %in% vNames) %>%
group_by(WordA,WordB) %>% tally()
# Calculate strength similarity
S.strength = cosineMatrix(G.strength[wordlist,])
S.ppmi = cosineMatrix(G.ppmi[wordlist,])
S.rw = cosineMatrix(G.rw[wordlist,])
X.sim = tibble('WordA' =pairs$WordA, 'WordB' = pairs$WordB,
'cos_strength' = S.strength[cbind(pairs$WordA,pairs$WordB)],
'cos_ppmi' = S.ppmi[cbind(pairs$WordA,pairs$WordB)],
'cos_rw' = S.rw[cbind(pairs$WordA,pairs$WordB)])
# The following lines of code are a bit more advanced and rely on the broom package.
# see https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html
# Basically we nest the data to apply a correlation test to each dataset, and
# tidy it so we get all test statistics in a clean dataframe.
# Add to the original ratings
X.results = inner_join(X.ratings,X.sim, by = c('WordA','WordB'))
X.r = X.results %>% nest(data = -dataset) %>%
mutate(r_strength = map(data, ~ cor.test(.x$Rating,.x$cos_strength)),
r_strength = map(r_strength, tidy)) %>%
mutate(r_ppmi = map(data, ~ cor.test(.x$Rating,.x$cos_ppmi)),
r_ppmi = map(r_ppmi, tidy)) %>%
mutate(r_rw = map(data, ~ cor.test(.x$Rating,.x$cos_rw)),
r_rw = map(r_rw, tidy))
library(broom)
# The following lines of code are a bit more advanced and rely on the broom package.
# see https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html
# Basically we nest the data to apply a correlation test to each dataset, and
# tidy it so we get all test statistics in a clean dataframe.
# Add to the original ratings
X.results = inner_join(X.ratings,X.sim, by = c('WordA','WordB'))
X.r = X.results %>% nest(data = -dataset) %>%
mutate(r_strength = map(data, ~ cor.test(.x$Rating,.x$cos_strength)),
r_strength = map(r_strength, tidy)) %>%
mutate(r_ppmi = map(data, ~ cor.test(.x$Rating,.x$cos_ppmi)),
r_ppmi = map(r_ppmi, tidy)) %>%
mutate(r_rw = map(data, ~ cor.test(.x$Rating,.x$cos_rw)),
r_rw = map(r_rw, tidy))
X.r = X.r %>% unnest(cols = c(r_strength,r_ppmi,r_rw), names_sep = '.')
print(X.r %>% mutate(r.strength = round(r_strength.estimate,3),
r.ppmi = round(r_ppmi.estimate,3),
r.rw = round(r_rw.estimate,3)) %>%
select(dataset,r.strength,r.ppmi,r.rw))
# Look up forward and backward associative strength
# Note: here we're indexing on a full graph, which means this needs to fit in memory
tmp.strength = as.array(igraph::as_adjacency_matrix(G.strength,attr='weight',names = TRUE))
tmp.ppmi = as.array(igraph::as_adjacency_matrix(G.ppmi,attr='weight',names = TRUE))
tmp.rw  = as.array(igraph::as_adjacency_matrix(G.rw,attr='weight',names = TRUE))
# Clean up the cbind
X.asso = tibble('WordA' =pairs$WordA, 'WordB' = pairs$WordB,
'fw_strength' = tmp.strength[cbind(pairs$WordA,pairs$WordB)],
'fw_ppmi' = tmp.ppmi[cbind(pairs$WordA,pairs$WordB)],
'fw_rw' = tmp.rw[cbind(pairs$WordA,pairs$WordB)],
'bw_strength' = tmp.strength[cbind(pairs$WordB,pairs$WordA)],
'bw_ppmi' = tmp.ppmi[cbind(pairs$WordB,pairs$WordA)],
'bw_rw' = tmp.rw[cbind(pairs$WordB,pairs$WordA)])
# Add to the original ratings
X.resultsA = inner_join(X.ratings,X.asso, by = c('WordA','WordB'))
# Ignore strength = 0
X.resultsA = X.resultsA %>% filter(fw_strength > 0, fw_ppmi > 0, fw_rw >0)
X.rFW = X.resultsA %>% nest(data = -dataset) %>%
mutate(fw_strength = map(data, ~ cor.test(.x$Rating,.x$fw_strength)),
fw_strength = map(fw_strength, tidy),
fw_ppmi = map(data, ~ cor.test(.x$Rating,.x$fw_ppmi)),
fw_ppmi = map(fw_ppmi, tidy),
fw_rw = map(data, ~ cor.test(.x$Rating,.x$fw_rw)),
fw_rw = map(fw_rw, tidy))
X.rFW = X.rFW %>% unnest(cols = c(fw_strength,fw_ppmi,fw_rw), names_sep = '.')
# Only keep datasets with at least 40 observations
print(X.rFW %>% mutate(r.strength = round(fw_strength.estimate,3),
r.ppmi = round(fw_ppmi.estimate,3),
r.rw = round(fw_rw.estimate,3)) %>%
select(dataset,parameter = fw_strength.parameter,r.strength,r.ppmi,r.rw) %>%
filter(parameter > 40))
# Add to the original ratings
X.resultsA = inner_join(X.ratings,X.asso, by = c('WordA','WordB'))
# Ignore strength = 0
X.resultsA = X.resultsA %>% filter(bw_strength > 0, bw_ppmi > 0, bw_rw >0)
X.rBW = X.resultsA %>% nest(data = -dataset) %>%
mutate(bw_strength = map(data, ~ cor.test(.x$Rating,.x$bw_strength)),
bw_strength = map(bw_strength, tidy),
bw_ppmi = map(data, ~ cor.test(.x$Rating,.x$bw_ppmi)),
bw_ppmi = map(bw_ppmi, tidy),
bw_rw = map(data, ~ cor.test(.x$Rating,.x$bw_rw)),
bw_rw = map(bw_rw, tidy))
X.rBW = X.rBW %>% unnest(cols = c(bw_strength,bw_ppmi,bw_rw), names_sep = '.')
# Only keep datasets with at least 40 observations
print(X.rBW %>% mutate(r.strength = round(bw_strength.estimate,3),
r.ppmi = round(bw_ppmi.estimate,3),
r.rw = round(bw_rw.estimate,3)) %>%
select(dataset,parameter = bw_strength.parameter,r.strength,r.ppmi,r.rw) %>%
filter(parameter > 40))
# Add to the original ratings
X.resultsA = inner_join(X.ratings,X.asso, by = c('WordA','WordB'))
# Ignore strength = 0
X.resultsA = X.resultsA %>% filter(bw_strength > 0, bw_ppmi > 0, bw_rw >0)
X.rBW = X.resultsA %>% nest(data = -dataset) %>%
mutate(bw_strength = map(data, ~ cor.test(.x$Rating,.x$bw_strength)),
bw_strength = map(bw_strength, tidy),
bw_ppmi = map(data, ~ cor.test(.x$Rating,.x$bw_ppmi)),
bw_ppmi = map(bw_ppmi, tidy),
bw_rw = map(data, ~ cor.test(.x$Rating,.x$bw_rw)),
bw_rw = map(bw_rw, tidy))
X.rBW = X.rBW %>% unnest(cols = c(bw_strength,bw_ppmi,bw_rw), names_sep = '.')
# Only keep datasets with at least 40 observations
print(X.rBW %>% mutate(r.strength = round(bw_strength.estimate,3),
r.ppmi = round(bw_ppmi.estimate,3),
r.rw = round(bw_rw.estimate,3)) %>%
select(dataset,parameter = bw_strength.parameter,r.strength,r.ppmi,r.rw) %>%
filter(parameter > 40))
# Make it fancy: http://haozhu233.github.io/kableExtra/use_kableExtra_with_formattable.html
G.rw['pasta','red']
# Neighboring vertices to pasta from the Katz Index graph G.rw
V.rw = data.frame(strength = G.rw['pasta',names(igraph::neighbors(G.rw,'pasta'))]) %>%
rownames_to_column(. , var = 'name') %>% as_tibble()
# Neighboring vertices to pasta from the PPMI graph G.ppmi
V.ppmi = data.frame(strength = G.ppmi['pasta',names(igraph::neighbors(G.ppmi,'pasta'))]) %>%
rownames_to_column(. , var = 'name')  %>% as_tibble()
# Combine to compare
V.joint = full_join(V.rw,V.ppmi,by  = 'name', suffix = c('.rw','.ppmi')) %>%
as_tibble() %>% select(name,strength.rw,strength.ppmi) %>% arrange(-strength.rw)
DT::datatable(V.joint)
# Calculate the joint probabilities
N_cue = USF %>% group_by(cue) %>% tally() %>% nrow()
N_resp = sum(USF$R1)
USF = USF %>% mutate(P_rc = R1.Strength / N_cue)
USF = USF %>% group_by(cue) %>% mutate(P_c = n / N_cue)
USF = USF %>% group_by(response) %>% mutate(n_R1 = sum(R1))
USF = USF %>% group_by(response) %>% mutate(P_r = n_R1/N_resp)
USF = USF %>% mutate(R1.PPMI = log2(P_rc/(P_c*P_r)))
USF = USF %>% select(-P_rc,-P_c,-n_R1,-P_r)
